TODO
Also save when submitting.
Merge teams.
Maybe register the submitter user, not only team
add filename preferences, filename table
leaderboards asc/desc
admin view of leaderboard should also contain new and failed models.
parallel contributivity
max submission name size
add precision for score (per ramp)
a being_trained boolean so we can output the next job to train, also taking into consideration that we should not take a model of somebody if her other model is being trained. It can be tricky is we manually kill jobs.
debug mode should raise the training/valid/test errors so we see the stack and set is_parallel to False
status of the queue
Didier's ideas:
  1) He wants to submit through git. What I understand is that he basically wants an API, not a web interface. 
  2) He pointed to us to jenkins: https://jenkins-ci.org/ for the engineering details of launching untrusted code in safe mode. That's what they use for automating their testing: gitlab and jenkins.
  3) He want spark integration.
  4) A general idea I didn't think of: we could have a fast evaluation using a subset of the data and a subset of the folds. Gradually increasing the number of folds when the server is less busy. So they don't have to wait an hour for running their code, or wait for other's jobs to finish. This is actually manageable. 
Fab helpers:
    get the path for a given submission to look at files